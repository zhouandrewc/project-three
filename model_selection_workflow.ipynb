{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model, svm, naive_bayes, neighbors, ensemble, metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from itertools import product\n",
    "import util\n",
    "from util.modeling import prepro, cross_val, get_scores, get_best_params, train_best_model, tune_hyper\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "X = data.drop(labels=[\"grad_bach\", \"survey_weight\"], axis=1)\n",
    "y = data[\"grad_bach\"]\n",
    "weights = data[\"survey_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there a way to select y_test so it's a good representation of the population?\n",
    "# if we select poorly it might be a bad one even weighted properly\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=0, stratify=y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.05, 0.1, 1, 5, 10, 100, 1000]\n",
    "solver = [\"liblinear\"]\n",
    "penalty = ['l1', 'l2']\n",
    "class_weight = [None, \"balanced\"]\n",
    "\n",
    "# fix oversampling w/ sample weights\n",
    "samplers = [None]+ [SMOTE(random_state=0, k_neighbors=k) for k in [3, 5, 7]]\n",
    "scale = [True]\n",
    "\n",
    "keywords = [\"C\", \"solver\", \"penalty\", \"class_weight\"]\n",
    "param_grid = product(C, solver, penalty, class_weight)\n",
    "\n",
    "prod_size = len(C)*len(solver)*len(penalty)*len(class_weight)*len(samplers)*len(scale)\n",
    "print_prog = (15, prod_size)\n",
    "\n",
    "sc_lr = tune_hyper(X_train, y_train, LogisticRegression, keywords, param_grid, weights, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_lr, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lr = [\"C\", \"solver\", \"penalty\", \"class_weight\"]\n",
    "lr_best = train_best_model(X_train, y_train, LogisticRegression, keywords_lr, sc_lr, \"fp5\", weights)\n",
    "cols = prepro(X_train)[0].columns\n",
    "lr_coefs = list(zip(cols, lr_best.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lr_coefs, key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_val, lr_best.predict(prepro(X_train, X_val, scale=True)[1]), sample_weight=weights[y_val.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.fbeta_score(y_val, lr_best.predict(prepro(X_train, X_val, scale=True)[1]), beta=0.5, sample_weight=weights[y_val.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100]\n",
    "criterion = [\"gini\"]#, \"entropy\"]\n",
    "#max_depth = [None, 3, 5]\n",
    "#min_samples_split = [2, 3]\n",
    "#min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "max_depth = [None, 3]\n",
    "min_samples_split = [2]\n",
    "min_samples_leaf [1]\n",
    "max_features = [\"auto\", \"log2\"]\n",
    "n_jobs = [-1]\n",
    "\n",
    "class_weight = [None, \"balanced\"]#, \"balanced_subsample\"]\n",
    "\n",
    "keywords = [\"n_estimators\", \"criterion\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"class_weight\", \"n_jobs\"]\n",
    "param_grid = product(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, class_weight, n_jobs)\n",
    "\n",
    "#samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [2, 3, 5, 7, 10]] + [ADASYN(random_state=0, n_neighbors=n) for n in [2, 3, 5, 7, 10]]\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [3, 5]]\n",
    "scale = [False]\n",
    "\n",
    "prod_size = len(n_estimators)*len(criterion)*len(max_depth)*len(min_samples_split)*len(min_samples_leaf)*len(max_features)*len(class_weight)*len(samplers)*len(scale)\n",
    "print_prog = (5, prod_size)\n",
    "\n",
    "sc_rf = tune_hyper(X_train, y_train, RandomForestClassifier, keywords, param_grid, weights, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_rf, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_rf = [\"n_estimators\", \"criterion\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"class_weight\", \"n_jobs\"]\n",
    "rf_best = train_best_model(X_train, y_train, RandomForestClassifier, keywords_rf, sc_rf, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = prepro(X_train)[0].columns\n",
    "rf_coefs = list(zip(cols, rf_best.feature_importances_))\n",
    "rf_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.fbeta_score(y_val, rf_best.predict(prepro(X_train, X_val, scale=False)[1]), beta=0.5, sample_weight=weights[y_val.index]))\n",
    "print(metrics.classification_report(y_val, rf_best.predict(prepro(X_train, X_val, scale=False)[1]), sample_weight=weights[y_val.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [5, 10, 20, 25, 35, 50, 75, 100]\n",
    "weights_knn = [\"uniform\", \"distance\"]\n",
    "algorithm = [\"auto\"]\n",
    "p = [1, 2, 3, 4]\n",
    "\n",
    "keywords = [\"n_neighbors\", \"weights\", \"algorithm\", \"p\"]\n",
    "param_grid = product(n_neighbors, weights_knn, algorithm, p)\n",
    "\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [3, 5, 7]]\n",
    "scale = [True]\n",
    "\n",
    "prod_size = len(n_neighbors)*len(weights_knn)*len(algorithm)*len(p)*len(samplers)*len(scale)\n",
    "print_prog = (10, prod_size)\n",
    "\n",
    "sc_knn = tune_hyper(X_train, y_train, KNeighborsClassifier, keywords, param_grid, weights, randomized=False, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_knn, \"fp5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.1, 0.5, 1, 5]\n",
    "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "gamma = [\"scale\", \"auto\"]\n",
    "class_weight = [None, \"balanced\"]\n",
    "probability = [False]\n",
    "\n",
    "keywords = [\"C\", \"kernel\", \"gamma\", \"class_weight\", \"probability\"]\n",
    "param_grid = product(C, kernel, gamma, class_weight, probability)\n",
    "\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [5, 7]]\n",
    "scale = [True]\n",
    "\n",
    "prod_size = len(C)*len(kernel)*len(gamma)*len(class_weight)*len(samplers)*len(scale)\n",
    "print_prog = (1, prod_size)\n",
    "\n",
    "sc_svc = tune_hyper(X_train, y_train, SVC, keywords, param_grid, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_svc, \"fp5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [25, 50, 100, 150]\n",
    "learning_rate = [1, 0.8, 0.5]\n",
    "keywords = [\"n_estimators\", \"learning_rate\"]\n",
    "param_grid = product(n_estimators, learning_rate)\n",
    "\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [2, 3, 5, 7, 10]] + [ADASYN(random_state=0, n_neighbors=n) for n in [2, 3, 5, 7, 10]]\n",
    "scale = [False]\n",
    "\n",
    "prod_size = len(n_estimators)*len(learning_rate)*len(samplers)*len(scale)\n",
    "print_prog = (10, prod_size)\n",
    "\n",
    "sc_ada = tune_hyper(X_train, y_train, AdaBoostClassifier, keywords, param_grid, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_ada, \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_scores_by_metric(sc_ada, \"f1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [30000]\n",
    "max_depth = [2, 3, 5, 7]\n",
    "objective = [\"reg:squarederror\", \"binary:logistic\", \"reg:gamma\", \"reg:tweedie\"]\n",
    "\n",
    "learning_rate =[0.05, 0.1, 0.15]\n",
    "subsample = [0.25, 0.5, 0.75, 1]\n",
    "colsample_bytree = [0.2, 0.4, 0.7, 1.0]\n",
    "lam = [0.1, 0.5, 1, 10, 100, 1000]\n",
    "    \n",
    "eval_metric = [\"logloss\"] \n",
    "\n",
    "\n",
    "max_depth = [2, 3]\n",
    "learning_rate = [0.1]\n",
    "lam = [0.5, 1, 5, 10, 100]\n",
    "subsample = [0.5, 0.8, 1]\n",
    "colsample_bytree = [0.5, 0.8, 1]\n",
    "\n",
    "keywords = [\"n_estimators\", \"max_depth\", \"objective\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"lambda\", \"eval_metric\"]\n",
    "param_grid = product(n_estimators, max_depth, objective, learning_rate, subsample, colsample_bytree, lam, eval_metric)\n",
    "\n",
    "\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [2, 3, 5, 7, 10]] + [ADASYN(random_state=0, n_neighbors=n) for n in [2, 3, 5, 7, 10]]\n",
    "scale = [False]\n",
    "\n",
    "samplers = [None] + [SMOTE(random_state=0, k_neighbors=k) for k in [5, 7]] + [ADASYN(random_state=0, n_neighbors=n) for n in [5, 7]]\n",
    "scale = [False]\n",
    "\n",
    "prod_size = len(n_estimators)*len(max_depth)*len(objective)*len(learning_rate)*len(subsample)*len(colsample_bytree)*len(lam)*len(samplers)\n",
    "print_prog = (100, prod_size)\n",
    "\n",
    "sc_xgb = tune_hyper(X_train, y_train, XGBClassifier, keywords, param_grid, samplers=samplers, scaling=scale, print_prog=print_prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = [\"n_estimators\", \"max_depth\", \"objective\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"lambda\", \"eval_metric\"]\n",
    "#sort_scores_by_metric(sc_xgb, \"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_xgb = [\"n_estimators\", \"max_depth\", \"objective\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"lambda\", \"eval_metric\"]\n",
    "#xgb_best = train_best_model(X_train, y_train, XGBClassifier, keywords_xgb, sc_xgb, \"f1\")\n",
    "# to train xgb i can either train-test-split or use their built-in cv function\n",
    "b_p = get_best_params(sc_xgb, \"f1\")\n",
    "clf_params, sampler, scale = b_p[0]\n",
    "\n",
    "X_tr, _ = prepro(X_train, scale=scale)\n",
    "if sampler:\n",
    "    X_tr, y_tr = sampler.fit_sample(X_tr, y_train)\n",
    "    \n",
    "kwargs = dict(zip(keywords_xgb[1:], clf_params[1:]))\n",
    "kwargs[\"random_state\"] = 0\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0)\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr.to_numpy())\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    kwargs,\n",
    "    dtrain,\n",
    "    num_boost_round=30000,\n",
    "    seed=0,\n",
    "    folds=skf,\n",
    "    metrics={'logloss'},\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=20\n",
    ")\n",
    "# is this not training properly for a classifier vs regressor? why is predict yielding numbers?\n",
    "# how do i make it into a scikitlearn compatible thing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb.train(kwargs, dtrain, num_boost_round=len(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lr = [\"C\", \"solver\", \"penalty\", \"class_weight\"]\n",
    "lr_best = train_best_model(X_train, y_train, LogisticRegression, keywords_lr, sc_lr, \"fp5\")\n",
    "keywords_rf = [\"n_estimators\", \"criterion\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"class_weight\", \"n_jobs\"]\n",
    "rf_best = train_best_model(X_train, y_train, RandomForestClassifier, keywords_rf, sc_rf, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#keywords_knn = [\"n_neighbors\", \"weights\", \"algorithm\", \"p\"]\n",
    "#knn_best = train_best_model(X_train, y_train, KNeighborsClassifier, keywords_knn, sc_knn, \"fp5\", randomized=False)\n",
    "keywords_svc = [\"C\", \"kernel\", \"gamma\", \"class_weight\", \"probability\"]\n",
    "svc_best = train_best_model(X_train, y_train, SVC, keywords_svc, sc_svc, \"fp5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lr = [\"C\", \"solver\", \"penalty\", \"class_weight\"]\n",
    "lr_best = train_best_model(X_train, y_train, LogisticRegression, keywords_lr, sc_lr, \"fp5\")\n",
    "keywords_rf = [\"n_estimators\", \"criterion\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"class_weight\", \"n_jobs\"]\n",
    "rf_best = train_best_model(X_train, y_train, RandomForestClassifier, keywords_rf, sc_rf, \"fp5\")\n",
    "\n",
    "keywords_knn = [\"n_neighbors\", \"weights\", \"algorithm\", \"p\"]\n",
    "knn_best = train_best_model(X_train, y_train, KNeighborsClassifier, keywords_knn, sc_knn, \"fp5\")\n",
    "\n",
    "keywords_svc = [\"C\", \"kernel\", \"gamma\", \"class_weight\", \"probability\"]\n",
    "svc_best = train_best_model(X_train, y_train, SVC, keywords_svc, sc_svc, \"fp5\")\n",
    "\n",
    "keywords_ada = [\"n_estimators\", \"learning_rate\"]\n",
    "ada_best = train_best_model(X_train, y_train, AdaBoostClassifier, keywords_ada, sc_ada, \"fp5\")\n",
    "\n",
    "keywords_xgb = [\"n_estimators\", \"max_depth\", \"objective\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"lambda\", \"eval_metric\"]\n",
    "xgb_best = train_best_model(X_train, y_train, XGBClassifier, keywords_xgb, sc_xgb, \"fp5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Models and Score Dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"models/lr_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(lr_best, pfile)\n",
    "#with open (f\"models/lr_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_lr, pfile)\n",
    "#with open(f\"models/rf_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(rf_best, pfile)\n",
    "#with open (f\"models/rf_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_rf, pfile)\n",
    "#with open(f\"models/knn_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(knn_best, pfile)\n",
    "#with open (f\"models/knn_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_knn, pfile)\n",
    "#with open(f\"models/svc_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(svc_best, pfile)\n",
    "#with open (f\"models/svc_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_svc, pfile)\n",
    "#with open(f\"models/ada_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(ada_best, pfile)\n",
    "#with open (f\"models/ada_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_ada, pfile)\n",
    "#with open(f\"models/xgb_best.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(xgb_best, pfile)\n",
    "#with open (f\"models/xgb_scores.pickle\", \"wb\") as pfile:\n",
    "#    pickle.dump(sc_xgb, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/lr_best.pickle\", \"rb\") as pfile:\n",
    "    lr_best = pickle.load(pfile)\n",
    "lr_best.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/rf_best.pickle\", \"rb\") as pfile:\n",
    "    rf_best = pickle.load(pfile)\n",
    "with open(f\"models/rf_scores.pickle\", \"rb\") as pfile:\n",
    "    sc_rf = pickle.load(pfile)\n",
    "with open(f\"models/lr_best.pickle\", \"rb\") as pfile:\n",
    "    lr_best = pickle.load(pfile)\n",
    "with open(f\"models/lr_scores.pickle\", \"rb\") as pfile:\n",
    "    sc_lr = pickle.load(pfile)    \n",
    "with open(f\"models/knn_best.pickle\", \"rb\") as pfile:\n",
    "    knn_best = pickle.load(pfile)\n",
    "with open(f\"models/svc_best.pickle\", \"rb\") as pfile:\n",
    "    svc_best = pickle.load(pfile)   \n",
    "with open(f\"models/ada_best.pickle\", \"rb\") as pfile:\n",
    "    ada_best = pickle.load(pfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ada_best.predict(prepro(X_train, X_val, scale=True)[1])\n",
    "print(metrics.fbeta_score(y_val, y_pred, beta=0.5))\n",
    "print(metrics.precision_score(y_val, y_pred))\n",
    "print(metrics.recall_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_lr, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_params(sc_lr, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.set(font_scale=2.8)\n",
    "sns.set_style(\"white\")\n",
    "metrics.plot_confusion_matrix(rf_best,prepro(X_train, X_val, scale=False)[1], y_val, sample_weight=weights[y_val.index], normalize=\"all\", cmap=plt.cm.Blues,ax=ax)\n",
    "ax.set_xlabel('Predicted Status', fontsize=44, labelpad=20);\n",
    "ax.set_ylabel('True Status', fontsize=44,labelpad=20);\n",
    "ax.set_title('Confusion Matrix, Random Forest Model', pad=40, fontsize=50);\n",
    "ax.xaxis.set_ticklabels([\"No Bachelor's\", \"Bachelor's\"], fontsize=40);\n",
    "ax.yaxis.set_ticklabels([\"No Bachelor's\", \"Bachelor's\"], rotation=90,fontsize=40, va=\"center\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_lr = lr_best.predict(prepro(X_train, X_val, scale=True)[1])\n",
    "print(metrics.classification_report(y_val, y_pred_lr))\n",
    "get_best_params(sc_lr, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_rf = rf_best.predict(prepro(X_train, X_val, scale=False)[1])\n",
    "print(metrics.classification_report(y_val, y_pred_rf, sample_weight=weights[y_val.index]))\n",
    "get_best_params(sc_rf, \"fp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.fbeta_score(y_val, y_pred_rf, beta=0.5, sample_weight=weights[y_val.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1+0.5*0.5)*(0.7444897*0.68576744)/(0.5*0.5*0.7444897+0.68576744)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.fbeta_score(y_val, y_pred_rf, beta=0.5, sample_weight=weights[y_val.index])\n",
    "metrics.precision_score(y_val, y_pred_rf, sample_weight=weights[y_val.index])\n",
    "metrics.recall_score(y_val, y_pred_rf, sample_weight=weights[y_val.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_rf[((100, 'gini', None, 2, 1, 'auto', None, -1), None, False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[172, 22],[39,58]]\n",
    "df_cm = pd.DataFrame(array, index = [i for i in [\"Bachelor's\", \"No Bachelor's\"]],\n",
    "              columns = [i for i in [\"Bachelor's\", \"No Bachelor's\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True,cmap=\"OrRd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = prepro(X_train)[0].columns\n",
    "lr_coefs = list(zip(cols, a.coef_[0]))\n",
    "pd.DataFrame(lr_coefs).to_csv('site/feature_importances.csv', header=True, index=False)\n",
    "\n",
    "pd.read_csv(\"site/feature_importances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"site/feature_importances.pickle\", \"wb\") as pfile:\n",
    "    pickle.dump(lr_coefs, pfile)\n",
    "with open(f\"site/feature_importances.pickle\", \"rb\") as pfile:\n",
    "    print(pickle.load(pfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Best Models and Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be best to make these pipelines that automatically scale or don't scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"lr_best\", \"rf_best\", \"knn_best\", \"svc_best\", \"ada_best\", \"xgb_best\"]\n",
    "to_scale = [True, False, True, True, False, False]\n",
    "models = []\n",
    "\n",
    "for model in model_names:\n",
    "    with open(f\"models/{model}.pickle\", \"rb\") as pfile:\n",
    "        exec(f\"{model} = pickle.load(pfile)\")\n",
    "        exec(f\"models.append({model})\")\n",
    "# make sure to check whether log is scaled - perhaps automate it from best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = prepro(X_train)[0].columns\n",
    "lr_coefs = list(zip(cols, lr_best.coef_[0]))\n",
    "lr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_coefs = list(zip(cols, svc_best.coef_[0]))\n",
    "svc_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x=\"region\", hue=\"grad_bach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_prepro_sc, X_v_prepro_sc = prepro(X_train, X_val, scale=True)\n",
    "X_tr_prepro, X_v_prepro = prepro(X_train, X_val)\n",
    "\n",
    "for model, model_name, scale in zip(models, model_names, to_scale):\n",
    "    X_v_select = X_v_prepro_sc if scale else X_v_prepro\n",
    "    if model_name == \"xgb_best\":\n",
    "        X_v_select = xgb.DMatrix(X_v_select)\n",
    "    y_pred_lr = model.predict(X_v_select)\n",
    "    print(model_name, \"\\n\", metrics.classification_report(y_val, np.round(y_pred_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = xgb_best.predict(xgb.DMatrix(X_v_prepro))\n",
    "print(metrics.classification_report(y_val, np.round(y_pred_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = xgb_best.predict(xgb.DMatrix(X_v_prepro))\n",
    "y_probs_lr = xgb_best.predict_proba(xgb.DMatrix(X_v_prepro))\n",
    "\n",
    "print(metrics.classification_report(y_train, xgb_best.predict(X_tr_prepro)))\n",
    "print(metrics.classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr_best.predict(X_v_prepro_sc)\n",
    "y_probs_lr = lr_best.predict_proba(X_v_prepro_sc)\n",
    "print(metrics.classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.plot_precision_recall_curve(lr_best, X_v_prepro_sc, y_val)\n",
    "sklearn.metrics.plot_roc_curve(lr_best, X_v_prepro_sc, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, th = metrics.precision_recall_curve(y_true, y_probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_best.predict(X_v_prepro)\n",
    "y_probs_rf = rf_best.predict_proba(X_v_prepro)\n",
    "print(metrics.classification_report(y_val, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.plot_precision_recall_curve(rf_best, X_v_prepro, y_val)\n",
    "sklearn.metrics.plot_roc_curve(rf_best, X_v_prepro, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = knn_best.predict(X_v_prepro_sc)\n",
    "y_probs_rf = knn_best.predict_proba(X_v_prepro_sc)\n",
    "print(metrics.classification_report(y_val, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# plot precision-recall curves for each best model\n",
    "# analyze feature importances\n",
    "# fix xgboost, retrain it as an sklearn-compatible model\n",
    "\n",
    "\n",
    "# workflow:\n",
    "# fix the preprocessing logic\n",
    "# initialize k-folds and samplers outside instead of making tons of them (may speed things up?)\n",
    "\n",
    "# extra:\n",
    "# perhaps make decision boundary plot\n",
    "# for final xgb training do i pass it an eval set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an easier way to just do X_val_prepro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
